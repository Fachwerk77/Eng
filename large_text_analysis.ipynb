{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Großtext-Analyse mit Limits\n",
    "\n",
    "Diese Notebook implementiert eine Lösung zur Analyse großer Textdateien unter Berücksichtigung von Speicherlimits.\n",
    "\n",
    "## Funktionalität\n",
    "- Verarbeitung großer Textdateien in Chunks\n",
    "- Speicherüberwachung und -management\n",
    "- Fortschrittsanzeige während der Verarbeitung\n",
    "- Konfigurierbare Chunk-Größe und Speichergrenzen\n",
    "\n",
    "## Limits\n",
    "- Maximale Chunk-Größe: 100MB\n",
    "- Speicherlimit: 80% der verfügbaren RAM\n",
    "- Maximale Dateigröße: Abhängig vom verfügbaren Speicher"
   ],
   "id": "27ca3dfb689faeab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import sys\n",
    "import psutil\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "from typing import Generator, Any\n",
    "\n",
    "# Konstanten\n",
    "MAX_CHUNK_SIZE = 1024 * 1024 * 100  # 100MB\n",
    "MEMORY_LIMIT_PERCENT = 80  # 80% des verfügbaren RAM"
   ],
   "id": "8e74aa885769a1d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TextChunkProcessor:\n",
    "    def __init__(self, chunk_size: int = MAX_CHUNK_SIZE):\n",
    "        self.chunk_size = chunk_size\n",
    "        self._memory_monitor = threading.Thread(target=self._monitor_memory, daemon=True)\n",
    "        self.should_stop = False\n",
    "\n",
    "    def process_file(self, filepath: str) -> Generator[str, Any, None]:\n",
    "        file_size = os.path.getsize(filepath)\n",
    "\n",
    "        with open(filepath, 'r') as file:\n",
    "            with tqdm(total=file_size, unit='B', unit_scale=True) as pbar:\n",
    "                chunk = file.read(self.chunk_size)\n",
    "                while chunk and not self.should_stop:\n",
    "                    yield chunk\n",
    "                    pbar.update(len(chunk))\n",
    "                    chunk = file.read(self.chunk_size)\n",
    "\n",
    "    def _monitor_memory(self):\n",
    "        while not self.should_stop:\n",
    "            if self._get_memory_usage() > MEMORY_LIMIT_PERCENT:\n",
    "                self.should_stop = True\n",
    "            time.sleep(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_memory_usage() -> float:\n",
    "        return psutil.Process().memory_percent()"
   ],
   "id": "6b10c596f01b8106"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def analyze_large_text(filepath: str) -> dict:\n",
    "    \"\"\"\n",
    "    Hauptfunktion zur Analyse großer Textdateien\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'word_count': 0,\n",
    "        'line_count': 0,\n",
    "        'char_count': 0,\n",
    "        'memory_usage': []\n",
    "    }\n",
    "\n",
    "    processor = TextChunkProcessor()\n",
    "    processor._memory_monitor.start()\n",
    "\n",
    "    try:\n",
    "        for chunk in processor.process_file(filepath):\n",
    "            results['word_count'] += len(chunk.split())\n",
    "            results['line_count'] += chunk.count('\\n')\n",
    "            results['char_count'] += len(chunk)\n",
    "            results['memory_usage'].append(processor._get_memory_usage())\n",
    "\n",
    "            if processor.should_stop:\n",
    "                print(\"Warnung: Speicherlimit erreicht!\")\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        processor.should_stop = True\n",
    "        processor._memory_monitor.join()\n",
    "\n",
    "    return results"
   ],
   "id": "bb89f68a4d32d8b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clear_memory():\n",
    "    \"\"\"\n",
    "    Speicherbereinigung durchführen\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "def get_available_memory() -> float:\n",
    "    \"\"\"\n",
    "    Verfügbaren Speicher ermitteln\n",
    "    \"\"\"\n",
    "    return psutil.virtual_memory().available / (1024 * 1024)  # MB\n",
    "\n",
    "def check_file_processable(filepath: str) -> bool:\n",
    "    \"\"\"\n",
    "    Prüft, ob die Datei verarbeitet werden kann\n",
    "    \"\"\"\n",
    "    file_size = os.path.getsize(filepath)\n",
    "    available_memory = get_available_memory()\n",
    "    return file_size < (available_memory * (MEMORY_LIMIT_PERCENT / 100))"
   ],
   "id": "a3efc1bfb3368aaa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Beispielverwendung\n",
    "if __name__ == \"__main__\":\n",
    "    beispiel_datei = \"große_textdatei.txt\"\n",
    "\n",
    "    if check_file_processable(beispiel_datei):\n",
    "        ergebnisse = analyze_large_text(beispiel_datei)\n",
    "        print(f\"Wortanzahl: {ergebnisse['word_count']}\")\n",
    "        print(f\"Zeilenanzahl: {ergebnisse['line_count']}\")\n",
    "        print(f\"Zeichenanzahl: {ergebnisse['char_count']}\")\n",
    "        print(f\"Durchschnittliche Speicherauslastung: {sum(ergebnisse['memory_usage'])/len(ergebnisse['memory_usage']):.2f}%\")\n",
    "    else:\n",
    "        print(\"Fehler: Datei zu groß für verfügbaren Speicher\")"
   ],
   "id": "95d4291f984e3723"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
